# -*- coding: utf-8 -*-
"""Expert Knowledge Worker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13oDrUiTLzN5O3lQUsxa35aAyPSUh5LVz

**Expert Knowledge Worker**

1.   A question answering agent that is an expert knowledge worker.
2.   To be used by employees of Insurellm, an Insurance Tech company.
3.   The agent needs to be accurate, and the solution should be low cost.

This project will use RAG (Retrieval Augmented Generation) to ensure Q&A assistant has high accuracy
"""

!pip install -q OpenAI
!pip install -q google-generativeai
!pip install -q python-dotenv
!pip install -q anthropic
!pip install -q gradio

# imports

import os
import os
import glob
from dotenv import load_dotenv
import gradio as gr
from openai import OpenAI
from IPython.display import Markdown, display, update_display
import sys

# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"

# Load environment variables in a file called .env

load_dotenv(override=True)
openai_api_key = os.getenv('OPENAI_API_KEY')
openai = OpenAI()

load_dotenv(override=True)
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')

# With massive thanks to student Dr John S. for fixing a bug in the below for Windows users!

context = {}

import glob

context = {}

#employees = glob.glob("knowledge-base/employees/*")
employees = glob.glob("/content/drive/MyDrive/Llms/llm_engineering/week5/knowledge-base/employees/*")

for employee in employees:                                              # Takes name of file, and last name, and puts dictionay key as name
    name = employee.split(' ')[-1][:-3]
    doc = ""
    with open(employee, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc
    name = employee.split(' ')[-1][:-3]
    doc = ""
    with open(employee, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc

context.keys()        # To look at all keys

context["Lancaster"]  # To look at CEO (Lancaster) key

products = glob.glob("knowledge-base/products/*")

for product in products:
    name = product.split(os.sep)[-1][:-3]
    doc = ""
    with open(product, "r", encoding="utf-8") as f:
        doc = f.read()
    context[name]=doc

context.keys()

system_message = "You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context."

def get_relevant_context(message):                              # This function will take the message, iterate through the context (title & details) in context such as last name of employee or product name (such as Lancaster)
    relevant_context = []
    for context_title, context_details in context.items():
        if context_title.lower() in message.lower():
            relevant_context.append(context_details)
    return relevant_context

get_relevant_context("Who is lancaster?")

get_relevant_context("Who is Avery and what is carllm?")

def add_context(message):                                       #  Take all information in the relevant context and add into message prompt
    relevant_context = get_relevant_context(message)
    if relevant_context:
        message += "\n\nThe following additional context might be relevant in answering this question:\n\n"
        for relevant in relevant_context:
            message += relevant + "\n\n"
    return message

print(add_context("Who is Alex Lancaster?"))

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history
    message = add_context(message)
    messages.append({"role": "user", "content": message})

    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        yield response

def chat(message, history):
    messages = [{"role": "system", "content": system_message}]
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})

    # Add context
    message_with_context = add_context(message)
    messages.append({"role": "user", "content": message_with_context})

    # Call OpenAI
    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

    response = ""
    for chunk in stream:
        content = chunk.choices[0].delta.content or ''
        response += content

    return response

"""## Now we will bring this up in Gradio using the Chat interface -

A quick and easy way to prototype a chat with an LLM
"""

with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Ask something...")
    state = gr.State([])

    def user_and_bot(message, history):
        messages = [{"role": "system", "content": system_message}]
        for user_msg, bot_msg in history:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": bot_msg})

        message_with_context = add_context(message)
        messages.append({"role": "user", "content": message_with_context})

        stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)

        partial = ""
        for chunk in stream:
            content = chunk.choices[0].delta.content or ""
            partial += content
            yield history + [[message, partial]], state

    msg.submit(user_and_bot, inputs=[msg, state], outputs=[chatbot, state])

demo.launch()