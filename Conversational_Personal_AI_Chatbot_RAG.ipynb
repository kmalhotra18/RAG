{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1L1XNiXsrfyjlKeaJzPmmpRKLklGfIq5P",
      "authorship_tag": "ABX9TyMHN7EDVNXyY/oYFaApcPx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalhotra18/RAG/blob/main/Conversational_Personal_AI_Chatbot_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d79qTjokyLZK"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "#!pip install -q langchain chromadb unstructured openai tiktoken sentence-transformers\n",
        "\n",
        "!pip install -q OpenAI\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q anthropic\n",
        "!pip install -q gradio\n",
        "!pip install -q langchain-community # Install the langchain-community package\n",
        "!pip install -q langchain-openai\n",
        "!pip install -q chromadb\n",
        "!pip install -q langchain-chroma\n",
        "!pip install unstructured\n",
        "!pip install \"unstructured[doc]\"\n",
        "!pip install \"unstructured[pdf]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for langchain and Chroma and plotly\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE                                     # To visualize\n",
        "import plotly.graph_objects as go                                     # To plot\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from langchain.document_loaders import UnstructuredFileLoader"
      ],
      "metadata": {
        "id": "pRivWBpiyu-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# price is a factor for our company, so we're going to use a low cost model\n",
        "\n",
        "MODEL = \"gpt-4o-turbo\"\n",
        "db_name = \"vector_db\""
      ],
      "metadata": {
        "id": "9byiL-0_yxYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import glob\n",
        "\n",
        "load_dotenv(override=True)\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
      ],
      "metadata": {
        "id": "hJFAC8uIyzln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y libreoffice"
      ],
      "metadata": {
        "id": "y9_TtIKuo_96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "root_directory = \"/content/drive/MyDrive/Important Documents/My Resumes\"\n",
        "\n",
        "documents = []\n",
        "skipped_files = []\n",
        "\n",
        "# Walk through all files manually\n",
        "for filepath in Path(root_directory).rglob(\"*\"):\n",
        "    if filepath.is_file() and not filepath.suffix.lower().endswith((\".gdoc\", \".gsheet\", \".gslides\")):\n",
        "        try:\n",
        "            loader = UnstructuredFileLoader(str(filepath))\n",
        "            loaded_docs = loader.load()\n",
        "            documents.extend(loaded_docs)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Skipping {filepath.name}: {e}\")\n",
        "            skipped_files.append(filepath)\n",
        "\n",
        "# Feedback\n",
        "print(f\"\\n‚úÖ Loaded {len(documents)} documents.\")\n",
        "if skipped_files:\n",
        "    print(f\"‚ö†Ô∏è Skipped {len(skipped_files)} files due to errors.\")"
      ],
      "metadata": {
        "id": "KmhzwF0NyREd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "-ANpF1kLyRBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'doc_type' exists before accessing it, providing a default value if not\n",
        "doc_types = set(chunk.metadata.get('doc_type', 'unknown') for chunk in chunks)\n",
        "print(f\"Document types found: {', '.join(doc_types)}\")"
      ],
      "metadata": {
        "id": "w9UlLrWZzGk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "db_name = \"/content/chroma_db\"  # path to where your DB will be saved\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# Optional: delete if the DB already exists\n",
        "if os.path.exists(db_name):\n",
        "    print(\"üßπ Removing existing Chroma DB...\")\n",
        "    shutil.rmtree(db_name)\n",
        "\n",
        "# Build vectorstore from document chunks\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "\n",
        "print(f\"‚úÖ Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "id": "dDAQQkTgzGiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Personal AI Chatbot"
      ],
      "metadata": {
        "id": "IhXOZM-OzLJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install required libraries\n",
        "!pip install -q langchain chromadb unstructured openai tiktoken sentence-transformers\n",
        "!apt install poppler-utils  # for PDF parsing\n",
        "!pip install -q pypdf # for PDF parsing\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q gradio\n",
        "!pip install -q pdfminer.six\n",
        "!pip install -q pi_heif\n",
        "!pip install -q unstructured-inference\n",
        "!pip install numpy\n",
        "!pip install pdf2image\n",
        "!pip install -q python-docx\n",
        "!pip install -q \"unstructured[local-inference,ocr,pytesseract]\"\n",
        "!apt install -y poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "id": "qyULsbF7zNzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 2: Mount Google Drive and locate the folder\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "OiZi8hdazGY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path to your folder\n",
        "DOC_FOLDER = \"/content/drive/MyDrive/Important Documents/My Resumes\"\n",
        "assert os.path.exists(DOC_FOLDER), \"Important Documents folder not found. Check path.\""
      ],
      "metadata": {
        "id": "PfrXCRMW4syO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 3: Load and process documents using Unstructured\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from unstructured.partition.text import partition_text\n",
        "from pathlib import Path\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    docs = []\n",
        "    for file in Path(folder_path).rglob(\"*\"):\n",
        "        try:\n",
        "            if file.suffix.lower() == \".pdf\":\n",
        "                elements = partition_pdf(filename=str(file))\n",
        "            elif file.suffix.lower() == \".docx\":\n",
        "                elements = partition_docx(filename=str(file))\n",
        "            elif file.suffix.lower() == \".txt\":\n",
        "                elements = partition_text(filename=str(file))\n",
        "            else:\n",
        "                continue\n",
        "            doc_text = \"\\n\".join([str(el) for el in elements])\n",
        "            docs.append({\"path\": str(file), \"text\": doc_text})\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Skipping {file.name}: {e}\")\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "vktbgz2N4su3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 4: Embed and store in Chroma vector store\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n"
      ],
      "metadata": {
        "id": "ppoqCTkM4sr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load HF embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "oRtvkt-o4soy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare langchain Documents\n",
        "raw_docs = load_documents(DOC_FOLDER) # Assuming DOC_FOLDER is the variable holding the path to your documents\n",
        "langchain_docs = [Document(page_content=doc['text'], metadata={\"source\": doc['path']}) for doc in raw_docs]"
      ],
      "metadata": {
        "id": "72YGr8jt40Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chroma DB\n",
        "persist_dir = \"/content/chroma_store\"\n",
        "vectordb = Chroma.from_documents(documents=langchain_docs, embedding=embedding_model, persist_directory=persist_dir)\n",
        "vectordb.persist()\n",
        "print(\"‚úÖ Vector store created and saved.\")"
      ],
      "metadata": {
        "id": "bXQ6lr9d40Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 5: Build RAG Q&A pipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import gradio as gr\n",
        "import glob"
      ],
      "metadata": {
        "id": "xkUYsKLC40JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-anthropic"
      ],
      "metadata": {
        "id": "xwIdXZvWBCyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "cu2D7dRXA-ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})  # Limit number of chunks\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "#llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\n",
        "\n",
        "load_dotenv(override=True)\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"map_reduce\"\n",
        ")"
      ],
      "metadata": {
        "id": "q75fnHqi9hWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Step 6: Ask a question\n",
        "query = \"What is the contract duration in the consulting agreement?\"\n",
        "response = qa.run(query)\n",
        "print(\"\\n‚ùì Question:\", query)\n",
        "print(\"\\nüí° Answer:\", response)\n"
      ],
      "metadata": {
        "id": "fODunpfJ437j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 6: Gradio Interface\n",
        "\n",
        "def answer_question(question):\n",
        "    return qa.run(question)\n",
        "\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(placeholder=\"Ask a question about your documents...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG Chatbot\",\n",
        "    description=\"Ask questions about the documents in your 'Important Documents' folder\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "zgC_F7E_4344"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}